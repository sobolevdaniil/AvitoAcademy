{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Академия Аналитиков Авито"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Эконометрика"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Общий план курса:\n",
    "- Повторение теории вероятности и математической статистики\n",
    "- Классическая линейная регрессия\n",
    "- Множественная регрессия\n",
    "- Метрики качества регрессии и проверка гипотез, связанных с ней\n",
    "- Нарушение предпосылок теоремы Гаусса-Маркова\n",
    "- Модели бинарного выбора\n",
    "- **Временные ряды**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Занятие №10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На предыдущем занятии мы:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Изучили разные типы ошибок прогноза\n",
    "\n",
    "![image.png](https://media.makeameme.org/created/so-youre-telling-5b9b3d.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Познакомились с моделью FBProphet\n",
    "\n",
    "![image.png](https://miro.medium.com/max/1154/1*jxk0QhfTSMN2xk0U7G_kbw.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Повторение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T16:54:50.783143Z",
     "start_time": "2020-10-27T16:54:50.778141Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy as sp\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.datasets import longley\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_acf \n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "\n",
    "# Модельки\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "# Тесты\n",
    "from statsmodels.tsa.stattools import adfuller, kpss, acf\n",
    "\n",
    "import prophet as fp \n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from plotly import graph_objs as go\n",
    "\n",
    "from prophet.diagnostics import cross_validation\n",
    "from prophet.diagnostics import performance_metrics\n",
    "from prophet.plot import add_changepoints_to_plot\n",
    "\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger('cmdstanpy')\n",
    "logger.addHandler(logging.NullHandler())\n",
    "logger.propagate = False\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "# инициализируем plotly\n",
    "init_notebook_mode(connected = True)\n",
    "\n",
    "pd.set_option('display.max_columns', None) # Чтобы показывались все колонки\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adf_test(timeseries):\n",
    "    print ('Results of Dickey-Fuller Test:')\n",
    "    dftest = adfuller(timeseries, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    print (dfoutput, '\\n Null Hypothesis: The series has a unit root.')\n",
    "    \n",
    "def kpss_test(timeseries):\n",
    "    print ('Results of KPSS Test:')\n",
    "    kpsstest = kpss(timeseries, regression='c', nlags=\"auto\")\n",
    "    kpss_output = pd.Series(kpsstest[0:3], index=['Test Statistic','p-value','Lags Used'])\n",
    "    for key,value in kpsstest[3].items():\n",
    "        kpss_output['Critical Value (%s)'%key] = value\n",
    "    print (kpss_output, '\\n Null Hypothesis: The process is trend stationary.')\n",
    "\n",
    "# опишем функцию, которая будет визуализировать все колонки dataframe в виде line plot\n",
    "def plotly_df(df, title = ''):\n",
    "    data = []\n",
    "\n",
    "    for column in df.columns:\n",
    "        trace = go.Scatter(\n",
    "            x = df.index,\n",
    "            y = df[column],\n",
    "            mode = 'lines',\n",
    "            name = column\n",
    "        )\n",
    "        data.append(trace)\n",
    "\n",
    "    layout = dict(title = title, template='plotly_white')\n",
    "    fig = dict(data = data, layout = layout)\n",
    "    iplot(fig, show_link=False)\n",
    "    \n",
    "# Функция для получения данных 'mdape', 'mape', 'mtape' для модели по методу имитированных исторических прогнозов\n",
    "def perf_metrics_28d(fp_model):\n",
    "    fp_df_cv = cross_validation(fp_model, initial='730.25 days', period='28 days', horizon = '28 days', parallel=\"processes\")\n",
    "    res = performance_metrics(fp_df_cv,rolling_window = 1)\n",
    "    mape = np.mean(abs(fp_df_cv['y'] - fp_df_cv['yhat'])/ (fp_df_cv['y']+1))\n",
    "    res['mape']=mape\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## План занятия: \n",
    "- Добавление нового предиктора\n",
    "- Seasonal ARIMA (SARIMA)\n",
    "- Диагностика модели\n",
    "    - Diebold-Mariano test\n",
    "    - Model encompassing\n",
    "    - Model combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Данные с прошлого занятия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для нашего примера возьмем данные о продажах Walmart, крупнейшей в мире компании по выручке, для прогнозирования ежедневных продаж на следующие 28 дней. С кейсом можно озанкомиться на kaggle: [M5 Forecasting - Uncertainty](https://www.kaggle.com/c/m5-forecasting-uncertainty/overview)\n",
    "\n",
    "Данные охватывают магазины в трех штатах США (Калифорния, Техас и Висконсин) и включают в себя детализацию о товаре - отдел, категорию и информацию о точке продаж. Кроме того, есть поясняющие переменные, такие как день недели и особые события. Вместе этот набор данных можно использовать для повышения точности прогнозов.\n",
    "\n",
    "Для нас это замечательный пример реальной потребности бизнеса в прогнозах продаж. Для себя мы ее немного упростим и займемся только **предсказанием количества проданных товаров в одном штате в одной категории в целом** (не углубляясь до торговых точек)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T23:02:22.043140Z",
     "start_time": "2020-10-26T23:02:10.601161Z"
    }
   },
   "outputs": [],
   "source": [
    "# Получение и обработка исходных данных.\n",
    "\n",
    "# Тренировочный датафрейм с проданными айтемами\n",
    "sales_train_evaluation = pd.read_csv('/home/aavoronkin/Projects/AAA/Class_9/sales_train_evaluation.csv')\n",
    "\n",
    "# Будем работать не с каждой конкретной точкой, а с продажами в каждом конкретном штате по категориям\n",
    "stv_cols = [c for c in sales_train_evaluation.columns if c not in ['id','item_id','dept_id','store_id']]\n",
    "sales_train_evaluation = sales_train_evaluation[stv_cols].groupby(['cat_id','state_id'], as_index = False).sum()\n",
    "\n",
    "# Преобразуем данные в удобный формат\n",
    "stv_cols_tr = [c for c in stv_cols if c not in ['cat_id','state_id']]\n",
    "sales_train_evaluation = pd.melt(sales_train_evaluation, id_vars = ['cat_id','state_id'], \\\n",
    "                                value_vars = stv_cols_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cправочник дат. SNAP - индикатор участия магазина в программе льготной покупки продуктов (Supplemental Nutrition Assistance Program)\n",
    "calendar = pd.read_csv('/home/aavoronkin/Projects/AAA/Class_9/calendar.csv', parse_dates = ['date'])\n",
    "calendar['date'] = calendar.date.dt.date\n",
    "calendar = calendar[['date', 'd', 'event_name_1','event_type_1','event_name_2','event_type_2', \\\n",
    "                     'snap_CA', 'snap_TX', 'snap_WI']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Присоединение дат к id\n",
    "sales_train_evaluation.columns = ['cat_id', 'state_id', 'd', 'y']\n",
    "sales_train_evaluation = sales_train_evaluation.merge(calendar[['d', 'date']], how = 'left', on = 'd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Приведение к нужному для fbprophet виду\n",
    "sales_train_evaluation = sales_train_evaluation[['date', 'state_id', 'cat_id', 'y']]\n",
    "sales_train_evaluation.columns = ['ds', 'state_id', 'cat_id', 'y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T23:03:04.127758Z",
     "start_time": "2020-10-26T23:03:03.102920Z"
    }
   },
   "outputs": [],
   "source": [
    "# Выделим только данные по Калифорнии\n",
    "ca_data = sales_train_evaluation.loc[sales_train_evaluation.state_id == 'CA', ['ds', 'cat_id', 'y']].copy()\n",
    "\n",
    "# Посмотрим на распрделение продаж по категориям\n",
    "plotly_df(ca_data.pivot(index='ds', columns='cat_id', values='y'), title = 'CA items sold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T23:03:07.110382Z",
     "start_time": "2020-10-26T23:03:07.100403Z"
    }
   },
   "outputs": [],
   "source": [
    "# Для нашего примера поработаем только с категорией 'HOUSEHOLD'\n",
    "\n",
    "data = ca_data.loc[(ca_data.cat_id == 'HOUSEHOLD')&(ca_data.ds<=datetime.date(2016,4,24)),['ds', 'y']].copy().reset_index(drop = True)\n",
    "\n",
    "data_evaluation = ca_data.loc[(ca_data.cat_id == 'HOUSEHOLD'),['ds', 'y']].copy().reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Календарь с праздниками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holidays\n",
    "\n",
    "us_ca_holidays = holidays.UnitedStates(state='CA', years = [2011,2012,2013,2014,2015,2016,2017,2018], \\\n",
    "                                       observed = False)\n",
    "\n",
    "holiday_df = pd.DataFrame.from_dict(us_ca_holidays, orient='index').reset_index(drop = False)\n",
    "holiday_df.columns = ['ds', 'holiday']\n",
    "holiday_df = holiday_df.sort_values('ds').reset_index(drop = True)\n",
    "holiday_df['lower_window'] = -1\n",
    "holiday_df['upper_window'] = 1\n",
    "holiday_df = holiday_df[['holiday', 'ds', 'lower_window', 'upper_window']]\n",
    "\n",
    "# В данных calendar в качестве событий были отмечены Пасха и День отца\n",
    "# Эти два регулярных события тоже могут отражаться на покупках (следует проверить данный факт отдельно)\n",
    "# Но сейчас в рамках нашей задачи мы так же вынесем эти дни + Valentine's Day в датафрейм с праздниками\n",
    "# В эти дни продажи отличаются от аналогичных обычных дней\n",
    "\n",
    "custom_holidays = pd.DataFrame([{'holiday' : \"Father's day\", 'ds' : datetime.date(2011,6,19), \\\n",
    "                                 'lower_window':-2, 'upper_window':0},\n",
    " {'holiday' : \"Father's day\", 'ds' : datetime.date(2012,6,17), 'lower_window':-1, 'upper_window':1},\n",
    " {'holiday' : \"Father's day\", 'ds' : datetime.date(2013,6,16), 'lower_window':-1, 'upper_window':1},\n",
    " {'holiday' : \"Father's day\", 'ds' : datetime.date(2014,6,15), 'lower_window':-1, 'upper_window':1},\n",
    " {'holiday' : \"Father's day\", 'ds' : datetime.date(2015,6,21), 'lower_window':-1, 'upper_window':1},\n",
    " {'holiday' : \"Father's day\", 'ds' : datetime.date(2016,6,19), 'lower_window':-1, 'upper_window':1},\n",
    " {'holiday' : \"Father's day\", 'ds' : datetime.date(2017,6,18), 'lower_window':-1, 'upper_window':1},\n",
    " {'holiday' : \"Father's day\", 'ds' : datetime.date(2018,6,17), 'lower_window':-1, 'upper_window':1},\n",
    " {'holiday' : \"Easter\", 'ds' : datetime.date(2011,4,24), 'lower_window':-1, 'upper_window':1},\n",
    " {'holiday' : \"Easter\", 'ds' : datetime.date(2012,4,15), 'lower_window':-1, 'upper_window':1},\n",
    " {'holiday' : \"Easter\", 'ds' : datetime.date(2013,5,5), 'lower_window':-1, 'upper_window':1},\n",
    " {'holiday' : \"Easter\", 'ds' : datetime.date(2014,4,20), 'lower_window':-1, 'upper_window':1},\n",
    " {'holiday' : \"Easter\", 'ds' : datetime.date(2015,4,12), 'lower_window':-1, 'upper_window':1},\n",
    " {'holiday' : \"Easter\", 'ds' : datetime.date(2016,5,1), 'lower_window':-1, 'upper_window':1},\n",
    " {'holiday' : \"Easter\", 'ds' : datetime.date(2017,4,16), 'lower_window':-1, 'upper_window':1},\n",
    " {'holiday' : \"Easter\", 'ds' : datetime.date(2018,4,8), 'lower_window':-1, 'upper_window':1},\n",
    " {'holiday' : \"Valentine's Day\", 'ds' : datetime.date(2011,2,14), 'lower_window':-1, 'upper_window':1},\n",
    " {'holiday' : \"Valentine's Day\", 'ds' : datetime.date(2012,2,14), 'lower_window':-1, 'upper_window':1},\n",
    " {'holiday' : \"Valentine's Day\", 'ds' : datetime.date(2013,2,14), 'lower_window':-1, 'upper_window':1},\n",
    " {'holiday' : \"Valentine's Day\", 'ds' : datetime.date(2014,2,14), 'lower_window':-1, 'upper_window':1},\n",
    " {'holiday' : \"Valentine's Day\", 'ds' : datetime.date(2015,2,14), 'lower_window':-1, 'upper_window':1},\n",
    " {'holiday' : \"Valentine's Day\", 'ds' : datetime.date(2016,2,14), 'lower_window':-1, 'upper_window':1},\n",
    " {'holiday' : \"Valentine's Day\", 'ds' : datetime.date(2017,2,14), 'lower_window':-1, 'upper_window':1},\n",
    " {'holiday' : \"Valentine's Day\", 'ds' : datetime.date(2018,2,14), 'lower_window':-1, 'upper_window':1}])\n",
    "\n",
    "holiday_df = pd.concat([holiday_df, custom_holidays])\n",
    "holiday_df = holiday_df.sort_values('ds').reset_index(drop = True)\n",
    "holiday_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Лучшая модель с прошлого занятия "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим, что будет при учете праздников\n",
    "\n",
    "m_7 = fp.Prophet(\n",
    "    changepoint_prior_scale=0.25,\n",
    "    seasonality_mode = 'multiplicative',\n",
    "    yearly_seasonality = False,\n",
    "    holidays = holiday_df)\n",
    "\n",
    "m_7.add_seasonality(name = 'yearly', \n",
    "                       period = 365.25,\n",
    "                       fourier_order = 3, \n",
    "                       mode = 'additive')\n",
    "\n",
    "m_7.add_seasonality(name = 'monthly', \n",
    "                       period = 30.5,\n",
    "                       fourier_order = 3, \n",
    "                       mode = 'additive')\n",
    "\n",
    "\n",
    "m_7.fit(data)\n",
    "\n",
    "future = m_7.make_future_dataframe(periods=365)\n",
    "\n",
    "forecast = m_7.predict(future)\n",
    "fig7 = m_7.plot(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "m7_metrics = perf_metrics_28d(m_7)\n",
    "m7_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Рассмотрим еще один вид ошибки:**\n",
    "\n",
    "$$\\text{ Mean absolute percentage error: }  MAPE = mean(\\frac{|y_t - \\hat{y_t}|}{y_t})$$\n",
    "\n",
    "$$\\text{ Symmetric mean absolute percentage error: }  SMAPE = 2mean(\\frac{200*|y_t - \\hat{y_t}|}{y_t+ \\hat{y_t}})$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# попробуем скорректировать влияние праздников\n",
    "\n",
    "m_8 = fp.Prophet(\n",
    "    changepoint_prior_scale=0.25,\n",
    "    seasonality_mode = 'multiplicative',\n",
    "    yearly_seasonality = False,\n",
    "    holidays = holiday_df,\n",
    "    holidays_prior_scale = 0.05)\n",
    "\n",
    "m_8.add_seasonality(name = 'yearly', \n",
    "                       period = 365.25,\n",
    "                       fourier_order = 3, \n",
    "                       mode = 'additive')\n",
    "\n",
    "m_8.add_seasonality(name = 'monthly', \n",
    "                       period = 30.5,\n",
    "                       fourier_order = 3, \n",
    "                       mode = 'additive')\n",
    "\n",
    "\n",
    "m_8.fit(data)\n",
    "\n",
    "future = m_8.make_future_dataframe(periods=365)\n",
    "\n",
    "forecast = m_8.predict(future)\n",
    "fig7 = m_8.plot(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "m8_metrics = perf_metrics_28d(m_8)\n",
    "m8_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Добавление нового предиктора"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дополнительные регрессоры могут быть добавлены к линейной части модели с использованием add_regressor метода. \n",
    "\n",
    "Столбец со значением регрессора должен присутствовать как во фреймах данных, так и в прогнозе. \n",
    "\n",
    "На графике компонентов этот эффект будет отображаться на графике extra_regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Попробуем добавить дополнительный регрессор 10 июня 2012 (там можно наблюдать \"ступеньку\" в данных)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T07:38:49.479682Z",
     "start_time": "2020-10-27T07:38:49.287678Z"
    }
   },
   "outputs": [],
   "source": [
    "# Посмотрим еще раз на первоначальные данные\n",
    "plotly_df(data.set_index('ds'), title = 'CA items sold in household category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T23:21:51.353763Z",
     "start_time": "2020-10-26T23:21:51.349740Z"
    }
   },
   "outputs": [],
   "source": [
    "# Просто проставляем 1, если данные после начала запуска производства в РФ и 0 в иных случаях\n",
    "\n",
    "def period_after(ds):\n",
    "    if ds >= datetime.date(2012,6,9):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T23:21:51.364734Z",
     "start_time": "2020-10-26T23:21:51.355770Z"
    }
   },
   "outputs": [],
   "source": [
    "# Добавляем и в обучающие данные и в future значение дополнительного регрессора \n",
    "\n",
    "data['after_something'] = data['ds'].apply(period_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T23:22:06.619514Z",
     "start_time": "2020-10-26T23:21:51.383736Z"
    }
   },
   "outputs": [],
   "source": [
    "# попробуем скорректировать влияние праздников\n",
    "\n",
    "m_9 = fp.Prophet(\n",
    "    changepoint_prior_scale=0.25,\n",
    "    seasonality_mode = 'multiplicative',\n",
    "    yearly_seasonality = False,\n",
    "    holidays = holiday_df,\n",
    "    holidays_prior_scale = 0.05)\n",
    "\n",
    "m_9.add_seasonality(name = 'yearly', \n",
    "                       period = 365.25,\n",
    "                       fourier_order = 3, \n",
    "                       mode = 'additive')\n",
    "\n",
    "m_9.add_seasonality(name = 'monthly', \n",
    "                       period = 30.5,\n",
    "                       fourier_order = 3, \n",
    "                       mode = 'additive')\n",
    "\n",
    "# Добавляем регрессор до fit\n",
    "m_9.add_regressor('after_something', mode='multiplicative')\n",
    "\n",
    "m_9.fit(data)\n",
    "\n",
    "future = m_9.make_future_dataframe(periods=365)\n",
    "future['after_something'] = future['ds'].apply(period_after)\n",
    "\n",
    "forecast = m_9.predict(future)\n",
    "figure = m_9.plot(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T23:22:08.974474Z",
     "start_time": "2020-10-26T23:22:06.621514Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig_components = m_9.plot_components(forecast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "m9_metrics = perf_metrics_28d(m_9)\n",
    "m9_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сравним с показателями модели с sesonal_mode = 'multiplicative'\n",
    "m8_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Попробуйте добавить дополнительный регрессор SNAP.** SNAP - индикатор участия магазина в программе льготной покупки продуктов (Supplemental Nutrition Assistance Program). Информацию про SNAP можно найти в таблице `calendar`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.merge(calendar[['date','snap_CA']], how = 'left', left_on='ds',\n",
    "                                                      right_on = 'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# попробуем скорректировать влияние праздников\n",
    "\n",
    "m_10 = fp.Prophet(\n",
    "    changepoint_prior_scale=0.25,\n",
    "    seasonality_mode = 'multiplicative',\n",
    "    yearly_seasonality = False,\n",
    "    holidays = holiday_df,\n",
    "    holidays_prior_scale = 0.05)\n",
    "\n",
    "m_10.add_seasonality(name = 'yearly', \n",
    "                       period = 365.25,\n",
    "                       fourier_order = 3, \n",
    "                       mode = 'additive')\n",
    "\n",
    "m_10.add_seasonality(name = 'monthly', \n",
    "                       period = 30.5,\n",
    "                       fourier_order = 3, \n",
    "                       mode = 'additive')\n",
    "\n",
    "# Добавляем регрессор до fit\n",
    "m_10.add_regressor('after_something', mode='multiplicative')\n",
    "m_10.add_regressor('snap_CA', mode='multiplicative')\n",
    "\n",
    "m_10.fit(data)\n",
    "\n",
    "future = m_10.make_future_dataframe(periods=55)\n",
    "future['after_something'] = future['ds'].apply(period_after)\n",
    "future['ds'] = future['ds'].dt.date\n",
    "future = future.merge(calendar[['date','snap_CA']], how = 'left', left_on='ds',\n",
    "                                                      right_on = 'date')\n",
    "\n",
    "forecast = m_10.predict(future)\n",
    "figure = m_10.plot(forecast)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig10 = m_10.plot_components(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "m10_metrics = perf_metrics_28d(m_10)\n",
    "m10_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m8_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seasonal ARIMA\n",
    "\n",
    "Seasonal ARIMA - модель очень похожая на классическую ARIMA, за исключением того, что SARIMA позволяет так же моделировать и сезонные компоненты ARIMA. \n",
    "\n",
    "![image.png](https://miro.medium.com/max/1400/1*yxe5Sf5JSAROq-UVgMLtmA.png)\n",
    "\n",
    "Формула: \n",
    "\n",
    "![image.png](https://miro.medium.com/max/1400/1*0A5Y8uQlGcVe6QpOGonZpw.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала попробуем смоделировать обычную ARIMA на наших данных "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим еще раз на первоначальные данные\n",
    "plotly_df(data.set_index('ds')[['y']], title = 'CA items sold in household category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(plot_acf(data.y))\n",
    "print(plot_pacf(data.y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf_test(data.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpss_test(data.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['y_diff'] = data.y.diff().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим еще раз на первоначальные данные\n",
    "plotly_df(data.set_index('ds')[['y_diff']], title = 'CA items sold in household category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(plot_acf(data['y_diff'].dropna()))\n",
    "print(plot_pacf(data['y_diff'].dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adf_test(data['y_diff'].dropna()),'\\n')\n",
    "print(kpss_test(data['y_diff'].dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # С помощью автоматического перебора, подберем спецификацию ARIMA\n",
    "\n",
    "# import pmdarima as pm\n",
    "\n",
    "# # Create auto_arima model\n",
    "# model1 = pm.auto_arima(data.y, #time series\n",
    "#                       d=1, # difference order\n",
    "#                       max_p=6, # max value of p to test \n",
    "#                       max_q=6, # max value of p to test\n",
    "#                       information_criterion='aic', # used to select best mode\n",
    "#                       trace=True, # prints the information_criterion for each model it fits\n",
    "#                       error_action='ignore', # ignore orders that don't work\n",
    "#                       stepwise=True, # apply an intelligent order search\n",
    "#                       suppress_warnings=True) \n",
    "\n",
    "# # Print model summary\n",
    "# print(model1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_arima = ARIMA(data.y, order=(6, 1, 6))\n",
    "best_arima =  best_arima.fit()\n",
    "best_arima.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "ax = best_arima.resid.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(plot_acf(best_arima.resid))\n",
    "print(plot_pacf(best_arima.resid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acorr_ljungbox(best_arima.resid, return_df=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кажется, что обычная ARIMA не справляется с моделлированием, даже если мы включим много AR лагов (6).\n",
    "\n",
    "**А что если попробовать SARIMA?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['y_diff_7'] = data.y.diff().diff(7).dropna()\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим еще раз на первоначальные данные\n",
    "plotly_df(data.set_index('ds')[['y_diff_7']])\n",
    "print(plot_acf(data[['y_diff_7']]))\n",
    "print(plot_pacf(data.set_index('ds')[['y_diff_7']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adf_test(data.set_index('ds')[['y_diff_7']]),'\\n')\n",
    "print(kpss_test(data.set_index('ds')[['y_diff_7']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # С помощью автоматического перебора, подберем спецификацию ARIMA\n",
    "\n",
    "# import pmdarima as pm\n",
    "\n",
    "# # Create auto_arima model\n",
    "# model2 = pm.auto_arima(data.y, #time series\n",
    "#                       d=1, # difference order\n",
    "#                       D=1, # seasonal difference order\n",
    "#                       max_p = 2,\n",
    "#                       max_q = 2,                       \n",
    "#                       information_criterion='aic', # used to select best mode\n",
    "#                       m=7, # number of periods in each season\n",
    "#                       max_P = 5,\n",
    "#                       start_Q = 0,\n",
    "#                       max_Q=0,\n",
    "#                       seasonal=True, # enable sasonal component\n",
    "#                       trace=True, # prints the information_criterion for each model it fits\n",
    "#                       error_action='ignore', # ignore orders that don't work\n",
    "#                       stepwise=True, # apply an intelligent order search\n",
    "#                       suppress_warnings=True) \n",
    "\n",
    "# # Print model summary\n",
    "# print(model2.summary())\n",
    "\n",
    "# # result = ARIMA(5,1,0)(2,0,0)[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "best_arima = ARIMA(data.y, order=(1, 1, 1), seasonal_order=(7,1,0,7))\n",
    "best_arima =  best_arima.fit()\n",
    "best_arima.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(data_evaluation['ds'])-max(data['ds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_sarima = best_arima.predict()\n",
    "forecast_sarima\n",
    "\n",
    "\n",
    "forecast_test_sarima = best_arima.forecast(28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "ax = best_arima.resid.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(plot_acf(best_arima.resid))\n",
    "print(plot_pacf(best_arima.resid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acorr_ljungbox(best_arima.resid, return_df=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Ответьте на вопросы:**</span>\n",
    "\n",
    "- Какая спецификация SARIMA у модели для данных о продажах категории FOODS магазина в Техасе?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для нашего примера поработаем только с категорией 'HOUSEHOLD'\n",
    "\n",
    "# Выделим только данные по Калифорнии\n",
    "tx_data = sales_train_evaluation.loc[sales_train_evaluation.state_id == 'TX', ['ds', 'cat_id', 'y']].copy()\n",
    "\n",
    "# Посмотрим на распрделение продаж по категориям\n",
    "plotly_df(tx_data.pivot(index='ds', columns='cat_id', values='y'), title = 'TX items sold')\n",
    "\n",
    "data_tx = ca_data.loc[(ca_data.cat_id == 'FOODS')&(ca_data.ds<=datetime.date(2016,4,24)),['ds', 'y']].copy().reset_index(drop = True)\n",
    "\n",
    "data_evaluation_tx = ca_data.loc[(ca_data.cat_id == 'FOODS'),['ds', 'y']].copy().reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выбор модели. Diebold-Mariano test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Допустим есть 2 модели: модель А и модель B. Тогда пусть условная разница ошибок моделей будет:\n",
    "\n",
    "$$e^A_t = y_t - y^A_t$$\n",
    "$$e^B_t = y_t - y^B_t$$\n",
    "$$d_t = (e^A_t)^2 - (e^B_t)^2$$\n",
    "\n",
    "\n",
    "Интуитивно, если модели одинаково хороши (или плохи), то $d_t$ будет около нуля. \n",
    "\n",
    "Однако, если же модель A лучше модели B, тогда $E(d_t)<0$\n",
    "\n",
    "Поэтому мы тестируем гипотезу: $H_0: E(d_t)=0$ против $H_a: E(d_t) \\neq 0$\n",
    "\n",
    "Ограничения теста:\n",
    " - модели не должны быть вложенными относительно друг друга \n",
    " - модели должны быть линейны"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# попробуем скорректировать влияние праздников\n",
    "\n",
    "m_6 = fp.Prophet(\n",
    "    changepoint_prior_scale=0.25,\n",
    "    seasonality_mode = 'multiplicative',\n",
    "    yearly_seasonality = False)\n",
    "\n",
    "m_6.add_seasonality(name = 'yearly', \n",
    "                       period = 365.25,\n",
    "                       fourier_order = 3, \n",
    "                       mode = 'additive')\n",
    "\n",
    "m_6.add_seasonality(name = 'monthly', \n",
    "                       period = 30.5,\n",
    "                       fourier_order = 3, \n",
    "                       mode = 'additive')\n",
    "\n",
    "m_6.fit(data)\n",
    "\n",
    "future = m_6.make_future_dataframe(periods=28)\n",
    "\n",
    "forecast_season_fp = m_6.predict(future)\n",
    "figure = m_6.plot(forecast_season_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_season_fp['ds'] = forecast_season_fp['ds'].dt.date\n",
    "data_w_error = data.merge(forecast_season_fp, how='left', on='ds')[['ds','y','yhat']]\n",
    "resid_prophet = (data_w_error.y-data_w_error.yhat).dropna()\n",
    "\n",
    "resid_sarima = best_arima.resid\n",
    "\n",
    "dt = ((resid_prophet)**2-(resid_sarima)**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "stats.ttest_1samp(dt, popmean=0, alternative='less')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Ответьте на вопросы:**</span>\n",
    "\n",
    "- Проверьте прогноз SARIMA относительно FBProphet для ваших данных Техаса. Можете использовать спецификацию, выбранную для модели калифорнии.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вложенность прогнозов\n",
    "\n",
    "*Прогноз B вложен в прогноз A, если прогноз А использует всю релевантную информацию, которую использует прогноз B (или даже больше).*\n",
    "\n",
    "Если же ни один из прогнозов не вложен друг в друга, значит, вероятно, комбинация этих прогнозов будет лучше, чем каждый из прогнозов по отдельности. \n",
    "\n",
    "Проверка на вложенность:\n",
    "\n",
    "- оцениваем простую линейную регрессию: $Y_t = \\beta_A*\\hat{Y^A_t}+\\beta_B*\\hat{Y^B_t}+e_t$\n",
    "- тестируем по очереди две гипотезы: \n",
    " - $H^A_0: \\beta_A = 1 \\space\\&\\space \\beta_B =0$ vs $H^A_1: \\beta_B \\neq 1 \\space\\&\\space \\beta_A \\neq 0$\n",
    " - $H^B_0: \\beta_B = 0 \\space\\&\\space \\beta_A = 1$ vs $H^A_1: \\beta_B \\neq 0 \\space\\&\\space \\beta_A \\neq 1$ \n",
    "- если $H^A_0$ не отвергается, значит прогноз B вложен в прогноз А. То есть достаточно использовать только прогноз А. \n",
    "- если обе гипотезы отвергаются, значит ни один из прогнозов не вложен друг в друга. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline model\n",
    "\n",
    "m = fp.Prophet()\n",
    "\n",
    "m.fit(data)\n",
    "\n",
    "future = m.make_future_dataframe(periods=28)\n",
    "\n",
    "forecast = m.predict(future)\n",
    "forecast_base = forecast[['ds','yhat']]\n",
    "forecast_base.columns=['ds', 'yhat_base']\n",
    "forecast_base['ds'] = forecast_base['ds'].dt.date\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = data.merge(forecast_season_fp, on ='ds',how ='left').merge(forecast_base, on ='ds',how ='left')\n",
    "\n",
    "data_test['yhat_sarima']=forecast_sarima\n",
    "\n",
    "data_test = data_test[['ds','y','yhat','yhat_base','yhat_sarima']]\n",
    "data_test.columns = ['ds','y','yhat_best_prophet','yhat_base_prophet','yhat_sarima']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = 'y ~ yhat_best_prophet + yhat_sarima'\n",
    "results = ols(formula, data_test).fit()\n",
    "hypotheses = '(yhat_best_prophet = 0), (yhat_sarima = 1)'\n",
    "f_test = results.f_test(hypotheses)\n",
    "print('Hypothesis {} testing results: '.format(hypotheses), f_test)\n",
    "\n",
    "formula = 'y ~ yhat_best_prophet + yhat_sarima'\n",
    "results = ols(formula, data_test).fit()\n",
    "hypotheses = '(yhat_best_prophet = 1), (yhat_sarima = 0)'\n",
    "f_test = results.f_test(hypotheses)\n",
    "print('Hypothesis {} testing results: '.format(hypotheses), f_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = 'y ~ yhat_best_prophet + yhat_base_prophet'\n",
    "results = ols(formula, data_test).fit()\n",
    "hypotheses = '(yhat_best_prophet = 0), (yhat_base_prophet = 1)'\n",
    "f_test = results.f_test(hypotheses)\n",
    "print('Hypothesis {} testing results: '.format(hypotheses), f_test)\n",
    "\n",
    "formula = 'y ~ yhat_best_prophet + yhat_base_prophet'\n",
    "results = ols(formula, data_test).fit()\n",
    "hypotheses = '(yhat_best_prophet = 1), (yhat_base_prophet = 0)'\n",
    "f_test = results.f_test(hypotheses)\n",
    "print('Hypothesis {} testing results: '.format(hypotheses), f_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Combination\n",
    "\n",
    "Комбинировать модели можно: \n",
    "- взвешивая их по ошибкам\n",
    "- оценивая веса с помощью линейной регрессии "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_fbprophet = ((data_test['y']-data_test['yhat_best_prophet'])**2).sum()\n",
    "\n",
    "rmse_sarima = ((data_test['y']-data_test['yhat_sarima'])**2).sum()\n",
    "\n",
    "w_prophet = rmse_fbprophet/(rmse_fbprophet+rmse_sarima)\n",
    "w_sarima = rmse_sarima/(rmse_fbprophet+rmse_sarima)\n",
    "print('prophet weight: {}, sarima weight: {}'.format(w_prophet,w_sarima))\n",
    "\n",
    "data_test['y_comb'] = data_test['yhat_best_prophet']* w_prophet + data_test['yhat_sarima']*w_sarima\n",
    "\n",
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = 'y ~ yhat_best_prophet + yhat_sarima'\n",
    "results = ols(formula, data_test).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
